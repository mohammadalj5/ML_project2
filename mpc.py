# -*- coding: utf-8 -*-
"""MPC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HQr1OjIqfkXxj7QRr2cp0qzMOoQECqDC
"""

# ======================================
# Classification Project - Healthcare
# ======================================

# 1. Libraries
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

# ======================================
# 2. Load Dataset
# ======================================
df = pd.read_csv("/content/Healthcare.csv")

# ======================================
# 3. Data Cleaning & Preprocessing
# ======================================

# Handle missing values
df = df.dropna()

# Remove duplicate records
df = df.drop_duplicates()

# ======================================
# 4. Features & Target
# ======================================
y = df["Disease"]                     # Target
X = df.drop(["Disease", "Patient_ID"], axis=1)


# Encode categorical features
X["Gender"] = LabelEncoder().fit_transform(X["Gender"])

# Encode target
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Number of classes
num_classes = len(np.unique(y))
print("Number of classes:", num_classes)

# Encode all categorical (object) columns
for col in X.select_dtypes(include=["object"]).columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# ======================================
# 5. Check Class Balance
# ======================================
print("Class distribution:")
print(pd.Series(y).value_counts())

# ======================================
# 6. Train / Validation / Test Split
# ======================================
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

# ======================================
# 7. Feature Scaling
# ======================================
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# ======================================
# 8. Evaluation Function
# ======================================
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision (macro):", precision_score(y_test, y_pred, average="macro"))
    print("Recall (macro):", recall_score(y_test, y_pred, average="macro"))
    print("F1-score (macro):", f1_score(y_test, y_pred, average="macro"))

# ======================================
# 9. Logistic Regression
# ======================================
lr = LogisticRegression(max_iter=500, class_weight="balanced")

param_lr = {"C": [0.1, 1, 10]}

grid_lr = GridSearchCV(lr, param_lr, cv=5, scoring="f1_macro")
grid_lr.fit(X_train, y_train)

print("\nLogistic Regression")
print("Best Parameters:", grid_lr.best_params_)
print("CV Score:", grid_lr.best_score_)
evaluate_model(grid_lr, X_test, y_test)

# ======================================
# 10. Decision Tree
# ======================================
dt = DecisionTreeClassifier(
    random_state=42,
    class_weight="balanced"
)

param_dt = {
    "max_depth": [5, 10, None],
    "min_samples_split": [2, 5, 10]
}

grid_dt = GridSearchCV(dt, param_dt, cv=5, scoring="f1_macro")
grid_dt.fit(X_train, y_train)

print("\nDecision Tree")
print("Best Parameters:", grid_dt.best_params_)
print("CV Score:", grid_dt.best_score_)
evaluate_model(grid_dt, X_test, y_test)

# ======================================
# 11. Random Forest
# ======================================
rf = RandomForestClassifier(
    random_state=42,
    class_weight="balanced"
)

param_rf = {
    "n_estimators": [100, 200],
    "max_depth": [None, 10]
}

grid_rf = GridSearchCV(rf, param_rf, cv=5, scoring="f1_macro")
grid_rf.fit(X_train, y_train)

print("\nRandom Forest")
print("Best Parameters:", grid_rf.best_params_)
print("CV Score:", grid_rf.best_score_)
evaluate_model(grid_rf, X_test, y_test)

# ======================================
# 12. Support Vector Machine
# ======================================
svc = SVC(class_weight="balanced")

param_svc = {
    "C": [1, 10],
    "kernel": ["rbf"]
}

grid_svc = GridSearchCV(svc, param_svc, cv=5, scoring="f1_macro")
grid_svc.fit(X_train, y_train)

print("\nSVM")
print("Best Parameters:", grid_svc.best_params_)
print("CV Score:", grid_svc.best_score_)
evaluate_model(grid_svc, X_test, y_test)

# ======================================
# 13. XGBoost Classifier
# ======================================
xgb = XGBClassifier(
    objective="multi:softmax",
    num_class=num_classes,
    eval_metric="mlogloss",
    random_state=42
)

param_xgb = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5],
    "learning_rate": [0.05, 0.1]
}

grid_xgb = GridSearchCV(xgb, param_xgb, cv=5, scoring="f1_macro")
grid_xgb.fit(X_train, y_train)

print("\nXGBoost")
print("Best Parameters:", grid_xgb.best_params_)
print("CV Score:", grid_xgb.best_score_)
evaluate_model(grid_xgb, X_test, y_test)