# -*- coding: utf-8 -*-
"""MPR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZlpC_uqOeumFi2W0J6yJmK9xhdfQ4DZo
"""

# ===============================
# Libraries
# ===============================
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

# ===============================
# Upload Dataset
# ===============================
df = pd.read_csv('/content/House_Price.csv')

# ===============================
# Dataset Overview
# ===============================
print(df.info())
print(df.describe())
print(df.isnull().sum())

# ===============================
# Handle Missing Values
# ===============================
df['n_hos_beds'] = df['n_hos_beds'].fillna(df['n_hos_beds'].mean())

# ===============================
# Remove Duplicates
# ===============================
df = df.drop_duplicates()

df['airport'] = df['airport'].map({'NO': 0, 'YES':1}).fillna(-1).astype(int)
df['bus_ter'] = df['bus_ter'].map({'NO': 0, 'YES':1}).fillna(-1).astype(int)
df['waterbody'] = df['waterbody'].map({'NAN': 0,'River':1,'Lake':2,'Lake and River': 3}).fillna(-1).astype(int)

df.head()

# ===============================
# Features & Target
# ===============================
X = df.drop('price', axis=1)
y = df['price']

# ===============================
# Target Analysis (Regression Balance Check)
# ===============================
print("Target Variable Statistics:")
print(y.describe())

# ===============================
# Train / Validation / Test Split
# ===============================
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42
)

# ===============================
# Scaling
# ===============================
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# ===============================
# 1. Linear Regression
# ===============================
lr = LinearRegression()
lr.fit(X_train, y_train)

y_train_pred = lr.predict(X_train)
y_test_pred = lr.predict(X_test)

print("\nLinear Regression")
print("Train R2:", r2_score(y_train, y_train_pred))
print("Test R2:", r2_score(y_test, y_test_pred))
print("MAE:", mean_absolute_error(y_test, y_test_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred)))

# ===============================
# 2. Decision Tree Regressor
# ===============================
dt = DecisionTreeRegressor(random_state=42)

param_grid_dt = {
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10]
}

grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='r2')
grid_dt.fit(X_train, y_train)

y_train_pred = grid_dt.predict(X_train)
y_test_pred = grid_dt.predict(X_test)

print("\nDecision Tree")
print("Best Params:", grid_dt.best_params_)
print("Best CV Score:", grid_dt.best_score_)
print("Train R2:", r2_score(y_train, y_train_pred))
print("Test R2:", r2_score(y_test, y_test_pred))
print("MAE:", mean_absolute_error(y_test, y_test_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred)))

# ===============================
# 3. Random Forest Regressor
# ===============================
rf = RandomForestRegressor(random_state=42)

param_grid_rf = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20, None]
}

grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='r2')
grid_rf.fit(X_train, y_train)

y_train_pred = grid_rf.predict(X_train)
y_test_pred = grid_rf.predict(X_test)

print("\nRandom Forest")
print("Best Params:", grid_rf.best_params_)
print("Best CV Score:", grid_rf.best_score_)
print("Train R2:", r2_score(y_train, y_train_pred))
print("Test R2:", r2_score(y_test, y_test_pred))
print("MAE:", mean_absolute_error(y_test, y_test_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred)))

# ===============================
# 4. Support Vector Regressor
# ===============================
svr = SVR()

param_grid_svr = {
    'C': [1, 10],
    'kernel': ['rbf'],
    'gamma': ['scale', 'auto']
}

grid_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring='r2')
grid_svr.fit(X_train, y_train)

y_train_pred = grid_svr.predict(X_train)
y_test_pred = grid_svr.predict(X_test)

print("\nSVR")
print("Best Params:", grid_svr.best_params_)
print("Best CV Score:", grid_svr.best_score_)
print("Train R2:", r2_score(y_train, y_train_pred))
print("Test R2:", r2_score(y_test, y_test_pred))
print("MAE:", mean_absolute_error(y_test, y_test_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred)))

# ===============================
# 5. XGBoost Regressor
# ===============================

xgb = XGBRegressor(
    objective='reg:squarederror',
    random_state=42
)

param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0]
}

grid_xgb = GridSearchCV(
    xgb,
    param_grid_xgb,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

grid_xgb.fit(X_train, y_train)

y_train_pred = grid_xgb.predict(X_train)
y_test_pred = grid_xgb.predict(X_test)

print("\nXGBoost Regressor")
print("Best Params:", grid_xgb.best_params_)
print("Best CV Score:", grid_xgb.best_score_)
print("Train R2:", r2_score(y_train, y_train_pred))
print("Test R2:", r2_score(y_test, y_test_pred))
print("MAE:", mean_absolute_error(y_test, y_test_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred)))